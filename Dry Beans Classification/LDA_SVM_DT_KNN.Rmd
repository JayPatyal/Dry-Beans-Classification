---
title: "Assigment_2"
author: "Jay"
date: "5/5/2022"
output: word_document
---

```{r}
library("readxl")
library(dplyr)
library(caTools)
library(caret)
library(pROC)
library(ROCit)
library(rpart)
library(rpart.plot)
library(Rborist)
library(class)
library(glmnet)
library(kernlab)
library(randomForest)
library(psych)
library(e1071)
library(party)
library(klaR)
library(psych)
library(MASS)
library(devtools)
library(Metrics, include.only = 'bias')
```

```{r}
library(Metrics, include.only = 'bias')
```


In the EDA we divided the datset in the ratio of 3:1. To calculate the variance of each model we will also use the same dataset and divide it in the ratio 3:2.

```{r}
data <- read_excel("Dry_Bean_Dataset.xlsx")
data
data= data[sample(1:nrow(data)), ]
summary(data)

data<- data[,-1]
head(data)
data$Class <- as.factor(data$Class)
set.seed(69)

data<- data[,-1]
set.seed(123)
ind <- sample(2, nrow(data),
              replace = TRUE,
              prob = c(0.75, 0.25))
training <- data[ind==1,]
testing <- data[ind==2,]

ind1 <- sample(2, nrow(data),
              replace = TRUE,
              prob = c(0.60, 0.40))
training2 <- data[ind1==1,]
testing2 <- data[ind1==2,]


```

```{r}
testing
```


## LINEAR Discriminant Analysis ##

After performing EDA on the dataset, we will perform LDA on the dataset. LDA is performed by creating a model of LDA using training data to train the model and it will be trained on the Class feature to identify the class. We load the LDA function from the MASS library. The tables for each prediction (training and testing) are created by using the table function. 


```{r}
lda.predict <- train(Class~., method = "lda", data = training)

lda_train <- predict(lda.predict, training)

lda_test <- predict(lda.predict, testing)

lda_train_tab <- table(Predicted = lda_train, Actual = training$Class)

lda_test_tab <- table(Predicted = lda_test, Actual = testing$Class)

```

The learning phase will be done on the training dataset and the generalization phase will be performed on the testing dataset. After performing the predictive analysis on the testing set, a confusion matrix is generated to give us an in-depth information of the model. The accuracy in the learning phase was 90.39% which means that the model’s learning ability is very good. The generalization phase accuracy was found out to be 90.38%  which is lower by only 0.01%. Since the accuracy of the two models is almost identical, we can say that the model’s fit is very good, and it is not overfitting.

By generating the confusion matrix for the generalization phase, we are also able to get the model’s specificity and sensitivity. The sensitivity of each class except ‘SIRA’ is more than 90%, whereas ‘SIRA’ has the sensitivity of 75%. ‘SIRA’ has a lot of overlapping features with ‘DERMASON’ as 123 instances have been classified as ‘DERMASON’ instead of ‘SIRA’. The specificity on the other hand is still above 90% for all the classes. 


```{r}

print('CONFUSION MATRIX FOR TRAINING SET')
confusionMatrix(training$Class, lda_train)
lda_tr_cm <- confusionMatrix(training$Class, lda_train)
#lda_tr_cm$overall['Accuracy']

print(" ")

print('CONFUSION MATRIX FOR TESTING SET')
confusionMatrix(as.factor(testing$Class), lda_test)
lda_tst_cm <- confusionMatrix(as.factor(testing$Class), lda_test)
```

From the confusion matrix and statistics, we can see that our kappa values are 0.88. According to Cohen’s interpretation, the closer the value is to 1 the better and kappa value as low as 0.41 is acceptable. This means our model has very few FN and FP.

The precision of the model in the Learning and Generalization phase is generated by the code below.
The precision of the learning phase is 93% and of generalization phase is 92.8%, meaning that there are very few false positives in our model. The only class with relatively low precision in both phases is 'SIRA'. As explained above 'SIRA' has overlapping features with other classes leading to misclassification. 

```{r}

lda_train_pre <- diag(lda_train_tab)/rowSums(lda_train_tab)
print('PRECISION FOR EACH CLASS IN TRAINING SET')
lda_train_pre
print(paste('Precision for training  set = ',mean(lda_train_pre)))


lda_test_pre <- diag(lda_test_tab)/rowSums(lda_test_tab)
print('PRECISION FOR EACH CLASS IN TESTING SET')
lda_test_pre
print(paste('Precision for testing set = ', mean(lda_test_pre)))

```

From the training and testing models we calculate the bias of each of them. The bias for generalization phase is 92.8% which is almost identical to the learning phase. The bias for the phases were 0.15 and 0.14 which is very low, meaning the model is not overfitting.

```{r}
print(paste('BIAS FOR TRAINING SET = ',bias(as.numeric(lda_train),as.numeric(training$Class))))
print(paste('BIAS FOR TESTING SET = ',bias(as.numeric(lda_test),as.numeric(testing$Class))))
```


Another performance measurement, AUC, was obtained using the pROC package in R. This package contains the multiclass.roc() function, which computes the ROC curves and AUC for multi-class classification models. Since this method implements a One-vs-One approach in generating the ROC curves, we will have 66 curves for our 12 classes. Figures 6 and 7 show the ROC curves for learning and generalization phases, respectively. The plots present ROC curves for 7 different class combinations out of the available 21, and the overall AUCs are provided in the output of the below code.

We compute the ROC curve and calculate its AUC for the Learning Phase in the code below:
```{r}

lda_train.roc = multiclass.roc(training$Class, as.numeric(lda_train))

lda_train_rs <- lda_train.roc[['rocs']]
plot.roc(main = '7 One vs One ROC Plot for learning  phase' ,lda_train_rs[[1]], lty=4)
sapply(2:7,function(i) lines.roc(lda_train_rs[[i]],col=i, lty=i))

legend("bottom", legend=c('1/2','1/3','1/4','1/5','1/6','1/7', '1/8'), col=1:7,
       lwd=2)
lda_train_auc <- (sapply(1:7,function(i) auc(lda_train_rs[[i]])))

legend("right", legend=c( 'auc = ', format(round(mean(lda_train_auc), 2), nsmall = 2)))


```

The mean of all the AUC's is computed below.  

```{r}
total_lda_tr_auc <- (sapply(1:length(lda_train_rs),function(i) auc(lda_train_rs[[i]])))
mean(total_lda_tr_auc)
```


We compute the ROC curve and calculate its AUC for the Generalization Phase in the code below:

```{r}
lda.roc <- multiclass.roc(testing$Class, as.numeric(lda_test))
lda_plot <- lda.roc[['rocs']]

lda_rs <- lda.roc[['rocs']]
plot.roc(main = '7 One vs One ROC Plot for Generalization phase' ,lda_rs[[1]], lty=4)

sapply(2:7,function(i) lines.roc(lda_rs[[i]],col=i, lty=i))

legend("bottom", legend=c('1/2','1/3','1/4','1/5','1/6','1/7', '1/8'), col=1:7,
       lwd=2)
lda_test_auc <- (sapply(1:7,function(i) auc(lda_rs[[i]])))
legend("right", legend=c( 'auc = ', format(round(mean(lda_test_auc), 2), nsmall = 2)))
```


The mean of all the AUC's is computed below.  

```{r}
total_lda_tst_auc <- (sapply(1:length(lda_rs),function(i) auc(lda_rs[[i]])))
mean(total_lda_tst_auc)
```

We Create a second model of LDA with 3:2 ratio data to find the variance in our dataset. The model is identical to the original model except for the training and testing dataset. We find the accuracy of the given model in the learning and generalization phase of the model. The accuracies are 90.2% and 90.7 respectively. The confusion matrix on the model with 60:40 split is created below.

```{r}
lda.predict2 <- train(Class~., method = "lda", data = training2)

lda_train2 <- (predict(lda.predict2, training2))
lda_train_tab2 <- table(Predicted = lda_train2, Actual = training2$Class)

lda2_tr_acc <- sum(diag(lda_train_tab2))/sum(lda_train_tab2)
lda2_tr_acc
lda_test2 <- (predict(lda.predict2, testing2))

lda_test_tab2 <- table(Predicted = lda_test2, Actual = testing2$Class)
lda2_tst_acc <- sum(diag(lda_test_tab2))/sum(lda_test_tab2)
lda2_tst_acc

confusionMatrix(as.factor(training2$Class), lda_train2)
confusionMatrix(as.factor(testing2$Class), lda_test2)
```


The small variance difference in our model shows that that the model is performing well even when the model is distributed in the ratio of 3:2.


```{r}
print(paste('Variance on the Training Dataset = ',lda_tr_cm$overall['Accuracy'] - lda2_tr_acc))
print(paste('Variance on the Testing Dataset = ',lda_tst_cm$overall['Accuracy'] - lda2_tst_acc))
```



### SVM MODEL ###

SVM is a supervised learning method used for classification, regression, and outliers’ detection. We usually apply SVM on high dimensional dataset or a dataset where number of dimensions is higher than number of samples. However, our current dataset doesn’t follow any of these conditions. We develop a SVM model by using the training data and start the learning phase on the dataset using the model. 

```{r}
svm_model <- svm(Class~., training, type = "C-classification")
svm_train <- predict(svm_model, training)

svm_tab <- table(Predicted = svm_train, Actual = training$Class)

svm_test <- predict(svm_model, testing)
svm_t_tab <- table(Predicted = svm_test, Actual = testing$Class)
```


By performing SVM on our learning phase, the accuracy comes out to be 93.1%. This is an excellent result on the training set. We test the model in the generalization phase. The accuracy of the generalization phase is 93.3 %. This means that our model learns well and can identify the Bean class very well. We plot the confusion matrix for both the training and the testing set to get the accuracy, specificity, and recall. The sensitivity and recall are both greater than 90% in all the classes meaning that the model can correctly identify the class of the bean more than 90%.

Trying SVM C-Classification for with down sample due to constrains on the computer which im working on this is only one possible The model can give good results as SVM is good with multiple dimensions.


```{r}
print('CONFUSION MATRIX FOR TRAINING SET')
confusionMatrix(training$Class,svm_train)
svm_tr_cm <-confusionMatrix(training$Class,svm_train)
#svm_tr_cm$overall['Accuracy']
print('CONFUSION MATRIX FOR TESTING SET')
confusionMatrix(svm_test, testing$Class)
svm_tst_cm <- confusionMatrix(svm_test, testing$Class)
```

```{r}
svm_train_pre <- diag(svm_tab)/rowSums(svm_tab)
print('PRECISION FOR EACH CLASS IN TRAINING SET')
svm_train_pre
print(paste('Precion for Training set = ',mean(svm_train_pre)))
svm_test_pre <- diag(svm_t_tab)/rowSums(svm_t_tab)
print('PRECISION FOR EACH CLASS IN TESTING SET')
svm_test_pre

print(paste('Precision for Testing set = ',mean(svm_test_pre)))

```

```{r}
print(paste('BIAS FOR TRAINING SET = ',bias(as.numeric(svm_train), as.numeric(training$Class))))
print(paste('BIAS FOR TRAINING SET = ',bias(as.numeric(svm_test),as.numeric(testing$Class))))

```

The ROC plot for sensitivity vs specificity for each Class is generated. We can observe that all but one plot is almost at 100%. The one class is ‘CALI’, whose accuracy is 78% and recall is 98.5%. We then calculate the area under the curve for all the ROC plots. Except for ‘CALI’ all classes have an AUC greater than 99%, whereas ‘CALI’ has an AUC of 95%.

The results were good especially the FN at down sample are good but still too high FP Not acceptable, The model is good with high dimensional classification Good in cases where there are clear boundaries seperating the positives and negatives and is memory efficient.

We compute the ROC curve and calculate its AUC for the Learning Phase in the code below:
```{r}
svm_train.roc = multiclass.roc(training$Class, as.numeric(svm_train))

svm_train_rs <- svm_train.roc[['rocs']]
plot.roc(main = '7 One vs One ROC Plot for learning  phase',svm_train_rs[[1]], lty=4)
svm_train_auc <- (sapply(1:7,function(i) auc(svm_train_rs[[i]])))

sapply(2:7,function(i) lines.roc(svm_train_rs[[i]],col=i, lty=i))

legend("bottom", legend=c('1/2','1/3','1/4','1/5','1/6','1/7','1/8'), col=1:7,
       lwd=2)
legend("right", legend=c('auc = ', format(round(mean(svm_train_auc), 2), nsmall = 2)))

```
We compute the ROC curve and calculate its AUC for the Generalization Phase in the code below:
```{r}
svm.roc = multiclass.roc(testing$Class, as.numeric(svm_test))

svm_rs <- svm.roc[['rocs']]
plot.roc(main = '7 One vs One ROC Plot for Generalization  phase',svm_rs[[1]], lty=4)

sapply(2:7,function(i) lines.roc(svm_rs[[i]],col=i, lty=i))
svm_test_auc <- (sapply(1:7,function(i) auc(svm_rs[[i]])))


legend("bottom", legend=c('1/2','1/3','1/4','1/5','1/6','1/7','1/8'), col=1:7,
       lwd=2)
legend("right", legend=c('auc = ',format(round(mean(svm_test_auc), 2), nsmall = 2)))
```

We create another model of the 3:2 ratio to find the variance of the model we have created. The confusion matrix on the model with 60:40 split is created below. The Kappa value and the accuracy of the 60:40 model is almost similar to the 75:25 split.

```{r}
svm_model2 <- svm(Class~., training2, type = "C-classification")

svm_train2 <- predict(svm_model2, training2)
svm_tab2 <- table(Predicted = svm_train2, Actual = training2$Class)
svm2_tr_acc <- sum(diag(svm_tab2))/sum(svm_tab2)

svm_test2 <- predict(svm_model2, testing2)
svm_t_tab2 <- table(Predicted = svm_test2, Actual = testing2$Class)
svm2_tst_acc <- sum(diag(svm_t_tab2))/sum(svm_t_tab2)

confusionMatrix(svm_train2, training2$Class)
confusionMatrix(svm_test2, testing2$Class)
```

```{r}
print(paste('Variance on the Training Dataset = ',svm_tr_cm$overall['Accuracy'] - svm2_tr_acc))
print(paste('Variance on the Testing Dataset = ',svm_tst_cm$overall['Accuracy'] - svm2_tst_acc))
```

## DECISION TREE ##

Decision tree is a probabilistic method which uses chance event outcomes, resource costs, and utility. It predicts the value of a target variable by learning simple decision rules inferred from the data features. In this dataset the developed model will learn from the features which is the best probabilistic method. After the training model is created using the training dataset we apply the model in the generalization phase. 

```{r}
dtree <- rpart(Class~.,data=training,method = 'class')
dtree_train <- predict(dtree,training,type = 'class')
dtree_test <- predict(dtree,testing,type = 'class')

```

```{r}
dtree_train_tab <- table(Predicted = dtree_train, Actual = training$Class)
dtree_test_tab <- table(Predicted = dtree_test, Actual = testing$Class)

dtree_tr_acc <- sum(diag(dtree_train_tab))/sum(dtree_train_tab)
dtree_tst_acc <- sum(diag(dtree_test_tab))/sum(dtree_test_tab)

```

The learning phase accuracy for the model is 87%. The accuracy for the generalization phase is 84.7%. It is lower than our learning phase accuracy. This is largely due to the misclassification of the class ‘SIRA’ and ‘DERMASON’ as well as ‘BARBUNYA’ and ‘CALI’. As we saw in the EDA process, the respective pairs have a lot of overlapping features and similarities. This has resulted in a lower accuracy for ‘SIRA’ at 79% and ‘CALI’ at 78%. The same pattern can be noticed in the learning phase where sensitivity for ‘CALI’ and ‘SIRA’ are 80% and 81%. While this sensitivity is not low, it is lower than the other classes. 
The probabilistic approach due to the overlapping similarity in both classes have resulted in a misclassification. However, when we check their specificity is over 95% in each case. This means that very few of the cases have returned as False Negative. 

```{r}

print('CONFUSION MATRIX FOR TRAINING SET')
confusionMatrix(dtree_train,training$Class)


print('CONFUSION MATRIX FOR TESTING SET')
confusionMatrix(dtree_test,testing$Class)

```


```{r}
prp(dtree, box.palette="Reds", tweak=1.2)
```


```{r}

dtree_train_pre <- diag(dtree_train_tab)/rowSums(dtree_train_tab)
dtree_test_pre <- diag(dtree_test_tab)/rowSums(dtree_test_tab)
print('PRECISION FOR EACH CLASS IN TRAINING SET')
dtree_train_pre
print(paste('Precision for Training Set = ',mean(dtree_train_pre)))
print('PRECISION FOR EACH CLASS IN TRAINING SET')
dtree_test_pre
print(paste('Precision for Training Set = ',mean(dtree_test_pre)))
```

```{r}

print(paste('BIAS FOR TRAINING SET = ' ,bias(as.numeric(dtree_train),as.numeric(training$Class))))
print(paste('BIAS FOR TESTING SET = ' ,bias(as.numeric(dtree_test),as.numeric(testing$Class))))
```

The ROC plot for sensitivity vs specificity for each Class is generated. We can observe that all but one plot is almost at 90%. The probabilistic nature of Decsion Tree is good, but due to similarities in some classes it still generates a few False Positives.

We compute the ROC curve and calculate its AUC for the Decsion Tree Model in the code below:
```{r}

dtree_train.roc <- multiclass.roc(training$Class, as.numeric(dtree_train))
dtree_train_rs <- dtree_train.roc[['rocs']]
plot.roc(main = '7 One vs One ROC Plot for Learning  phase',dtree_train_rs[[1]], lty=4)


dtree_train_auc <- (sapply(1:7,function(i) auc(dtree_train_rs[[i]])))
sapply(2:7,function(i) lines.roc(dtree_train_rs[[i]],col=i, lty=i))

legend("bottom", legend=c('1/2','1/3','1/4','1/5','1/6','1/7', '1/8'), col=1:7,
       lwd=2)
legend("right", legend=c('auc =',format(round(mean(dtree_train_auc), 2), nsmall = 2)))

```

```{r}
dtree.roc <- multiclass.roc(testing$Class, as.numeric(dtree_test))
#table(testing$Class)
dtree_rs <- dtree.roc[['rocs']]
plot.roc(main = '7 One vs One ROC Plot for Generalization  phase',dtree_rs[[1]], lty=4)

dtree_test_auc <- (sapply(1:7,function(i) auc(dtree_rs[[i]])))
sapply(2:7,function(i) lines.roc(dtree_rs[[i]],col=i, lty=i))

legend("bottom", legend=c('1/2','1/3','1/4','1/5','1/6','1/7', '1/8'), col=1:7,
       lwd=2)
legend("right", legend=c('auc = ',format(round(mean(dtree_test_auc), 2), nsmall = 2)))
```



We create another model of the 3:2 ratio to find the variance of the model we have created. The confusion matrix on the model with 60:40 split is created below.

The accuracy of this model is also similar to the original dataset that we chose. This means that the decision tree has been trained well and the relatively small training set does not lead to drastic change in the accuracy. The false positives and true negatives of the test model are also very few.

```{r}
dtree2 <- rpart(Class~.,data=training2,method = 'class')
dtree_train2 <- predict(dtree2,training2,type = 'class')
dtree_test2 <- predict(dtree2,testing2,type = 'class')

dtree_train_tab2 <- table(Predicted = dtree_train2, Actual = training2$Class)
dtree_test_tab2 <- table(Predicted = dtree_test2, Actual = testing2$Class)

dtree2_tr_acc <- sum(diag(dtree_train_tab2))/sum(dtree_train_tab2)
dtree2_tst_acc <- sum(diag(dtree_test_tab2))/sum(dtree_test_tab2)

confusionMatrix(dtree_train2,training2$Class)
confusionMatrix(dtree_test2,testing2$Class)
```

```{r}
prp(dtree2, box.palette="Reds", tweak=1.2)
```


We find the variance of our model which comes out 0.001 and 0.025. This means that our model has low variance and bias. We can conclude that our model works well on both sets of data and is not overfitting or underfitting.

```{r}
print(paste('Variance on the Training Dataset = ',dtree_tr_acc  - dtree2_tr_acc ))
print(paste('Variance on the Testing Dataset = ',dtree_tst_acc - dtree2_tst_acc ))
```



## KNN Classifier ##

K-nearest neighbors (kNN) is a non-parametric method for classification, meaning that there are no model parameters to estimate. Therefore, there is no learning phase with this algo- rithm, since it directly predicts the class of previously unseen data based on the available labeled observations. The knn() function from class package is used, which takes the train- ing set, its labels, and testing set as inputs, and returns the class predictions for the testing set. Since our number of classes is 7, the value of k should be an odd number larger than 7; hence; k was set as 13. The confusion matrix, performance metrics, and ROC curve are provided below.

```{r}
knn_model <- knn(train= training[1:14],test=testing[1:14],cl= training$Class,k=13)
knn_tab <- table(testing$Class,knn_model)
knn_acc <- sum(diag(knn_tab))/sum(knn_tab)
```

The kappa value of the model is relatively low as well as its accuracy as compared to the other models. This is simply due to the fact that we do not train the KNN model. As we saw in the EDA portion there are multiple classes with overlapping features, which causes the low accuracy. The precision for the class 'BARBUNYA' is around 30% and 'SEKER' is 60% which is a major reason for the low accuracy.

```{r}
print('CONFUSION MATRIX FOR KNN MODEL')
confusionMatrix(knn_model,testing$Class)

```

```{r}

knn_pre <- diag(knn_tab)/rowSums(knn_tab)
print('PRECISION FOR EACH CLASS')
knn_pre

print(paste('PRECISION FOR KNN MODEL = ',mean(knn_pre)))

print(paste('BIAS FOR KNN MODEL',bias(as.numeric(knn_model),as.numeric(testing$Class))))
```


```{r, echo=FALSE}
knn.roc <- multiclass.roc(testing$Class, as.numeric(knn_model))
knn_rs <- knn.roc[['rocs']]
plot.roc(main = '7 One vs One ROC Plot for KNN model', knn_rs[[1]], lty=4)

sapply(2:7,function(i) lines.roc(knn_rs[[i]],col=i, lty=i))
knn_auc <- (sapply(1:7,function(i) auc(knn_rs[[i]])))
knn_auc
mean(knn_auc)
legend("bottom", legend=c('1/2','1/3','1/4','1/5','1/6','1/7', '1/8'), col=1:7,
       lwd=2)
legend("right", legend=c('auc =',format(round(mean(knn_auc), 2), nsmall = 2)))

```
We create another model with training2 and testing2 data to find the variance of the model we have created.The accuracy 
```{r}
knn_model2 <- knn(train= training2[1:14],test=testing2[1:14],cl= training2$Class,k=13)
knn_tab2 <- table(testing2$Class,knn_model2)
knn_acc2 <- sum(diag(knn_tab2))/sum(knn_tab2)
confusionMatrix(knn_model2,testing2$Class)
```

We find the variance of our model which comes out 0.03. This means that our model has low variance and bias. We can conclude that our model works well on both sets of data and is not overfitting or underfitting.

We find the variance of our model which comes out 0.03. This means that our model has low variance and bias. We can conclude that our model works well on both sets of data and is not overfitting or underfitting.
KNN model for prediction of Dry Beans has been completed where the following points from the start have been accomplished: Minimize the False Negatives and False Positives and if possible, remove False Positives whatsoever Build a model that can be deployed for small dataset classification with very small amount of memory.

Along with that another important point has been acheived which by using KNN allows us to very importantly use a fact that model can be easily updated with real time classification that keeps occurring can be added up to training set in Real Time without delay which allows us to modify the model easily rather than building the models and updating them. 
The variance of the model is very low, proving that the model can be easily updated.
```{r}
print(paste('Variance of the KNN Model = ',knn_acc - knn_acc2 ))
```

''















